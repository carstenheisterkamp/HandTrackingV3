"""
DepthAI v3 Script Node: Dynamic ROI Extraction from Palm Detection
==================================================================
This script runs ON THE OAK-D device (Myriad X LEON CSS processor) to:
1. Parse Palm Detection SSD-MobileNet outputs (896 anchors)
2. Apply Non-Maximum Suppression (NMS)
3. Calculate optimal ROI for Hand Landmark model
4. Send dynamic ImageManip config for cropping

Performance: LEON CSS @ 600MHz, must stay <10ms per frame (30 FPS target)
"""

import math

# ============================================================
# SSD Anchor Configuration (Palm Detection Model)
# ============================================================
# The model outputs 2 tensors:
# - classificators: [1, 896, 1] (Confidence scores for "hand" class)
# - regressors:     [1, 896, 18] (Bounding box deltas + keypoints)

ANCHORS = [
    # Layer 1: 12x12 grid (Feature Stride 16) -> 144 anchors
    # Layer 2: 6x6 grid   (Feature Stride 32) -> 36 anchors
    # Layer 3: 4x4 grid   (Feature Stride 32) -> 16 anchors
    # Layer 4: 3x3 grid   (Feature Stride 32) -> 9 anchors
    # Layer 5: 2x2 grid   (Feature Stride 32) -> 4 anchors
    # Total: 209 spatial locations x 4 aspect ratios + some = 896
]

# Generate anchors (simplified for device constraints)
def generate_anchors():
    """Generate SSD anchor boxes for 192x192 input"""
    anchors = []
    # Feature map sizes for SSD-MobileNet @ 192x192
    feature_map_sizes = [12, 6, 4, 3, 2]
    aspect_ratios = [1.0]  # Square anchors for palm detection
    scales = [0.1484375, 0.75, 0.5, 0.25, 0.15]  # Approx from model

    for idx, fmap_size in enumerate(feature_map_sizes):
        scale = scales[idx]
        for y in range(fmap_size):
            for x in range(fmap_size):
                # Anchor center (normalized 0-1)
                cx = (x + 0.5) / fmap_size
                cy = (y + 0.5) / fmap_size

                for ratio in aspect_ratios:
                    # Anchor size (normalized)
                    w = scale * math.sqrt(ratio)
                    h = scale / math.sqrt(ratio)
                    anchors.append([cx, cy, w, h])

    return anchors

# Pre-generate anchors (runs once at startup)
ANCHOR_BOXES = generate_anchors()

# ============================================================
# Main Loop
# ============================================================
def decode_palm_detections(nn_data, conf_threshold=0.5):
    """
    Decode SSD outputs into bounding boxes

    Args:
        nn_data: NeuralNetwork output with layers
        conf_threshold: Minimum confidence (0.5 = 50%)

    Returns:
        List of detections: [(x_center, y_center, width, height, confidence), ...]
    """
    # Extract tensors (FP16 format from SH4 blob)
    try:
        # Layer 0: classificators [1, 896, 1]
        scores = nn_data.getLayerFp16("classificators")
        # Layer 1: regressors [1, 896, 18]
        boxes = nn_data.getLayerFp16("regressors")
    except:
        # Fallback: Try alternative layer names
        scores = nn_data.getFirstLayerFp16()
        boxes = nn_data.getAllLayersFp16()[1] if len(nn_data.getAllLayersFp16()) > 1 else []

    detections = []

    # Iterate over 896 anchors
    for i in range(min(len(scores), len(ANCHOR_BOXES))):
        conf = scores[i]

        if conf < conf_threshold:
            continue

        # Get anchor
        anchor = ANCHOR_BOXES[i]
        anchor_cx, anchor_cy, anchor_w, anchor_h = anchor

        # Decode bounding box (SSD-style delta decoding)
        # Regressors format: [dx, dy, dw, dh, kp1_x, kp1_y, ..., kp7_x, kp7_y]
        dx = boxes[i * 18 + 0]
        dy = boxes[i * 18 + 1]
        dw = boxes[i * 18 + 2]
        dh = boxes[i * 18 + 3]

        # Apply deltas to anchor
        cx = anchor_cx + dx * anchor_w * 0.1  # Variance scaling (typically 0.1)
        cy = anchor_cy + dy * anchor_h * 0.1
        w = anchor_w * math.exp(dw * 0.2)     # Variance scaling (typically 0.2)
        h = anchor_h * math.exp(dh * 0.2)

        # Clamp to [0, 1]
        cx = max(0.0, min(1.0, cx))
        cy = max(0.0, min(1.0, cy))
        w = max(0.01, min(1.0, w))
        h = max(0.01, min(1.0, h))

        detections.append((cx, cy, w, h, conf))

    return detections


def apply_nms(detections, iou_threshold=0.3):
    """
    Non-Maximum Suppression to filter overlapping boxes

    Args:
        detections: List of (cx, cy, w, h, conf)
        iou_threshold: IoU threshold for suppression (0.3 = 30% overlap)

    Returns:
        Filtered list of detections
    """
    if not detections:
        return []

    # Sort by confidence (highest first)
    detections = sorted(detections, key=lambda x: x[4], reverse=True)

    keep = []
    while detections:
        best = detections.pop(0)
        keep.append(best)

        # Filter overlapping boxes
        detections = [d for d in detections if compute_iou(best, d) < iou_threshold]

    return keep


def compute_iou(box1, box2):
    """Compute Intersection over Union"""
    cx1, cy1, w1, h1, _ = box1
    cx2, cy2, w2, h2, _ = box2

    # Convert center-size to corner coords
    x1_min, y1_min = cx1 - w1/2, cy1 - h1/2
    x1_max, y1_max = cx1 + w1/2, cy1 + h1/2
    x2_min, y2_min = cx2 - w2/2, cy2 - h2/2
    x2_max, y2_max = cx2 + w2/2, cy2 + h2/2

    # Intersection
    inter_xmin = max(x1_min, x2_min)
    inter_ymin = max(y1_min, y2_min)
    inter_xmax = min(x1_max, x2_max)
    inter_ymax = min(y1_max, y2_max)

    inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)

    # Union
    area1 = w1 * h1
    area2 = w2 * h2
    union_area = area1 + area2 - inter_area

    return inter_area / union_area if union_area > 0 else 0


def calculate_roi(detection, scale_factor=1.5):
    """
    Calculate ROI for Hand Landmark model

    Args:
        detection: (cx, cy, w, h, conf)
        scale_factor: Expand palm bbox by this factor (default 1.5x = 50% padding)

    Returns:
        (roi_cx, roi_cy, roi_size) normalized to [0, 1]
    """
    cx, cy, w, h, _ = detection

    # Expand to include full hand (palm detection only sees palm)
    roi_w = w * scale_factor
    roi_h = h * scale_factor

    # Make square (Landmark model expects 224x224)
    roi_size = max(roi_w, roi_h)

    # Shift center slightly upward (fingers extend above palm)
    roi_cy = cy - roi_size * 0.1  # Shift up by 10% of ROI size

    # Clamp to image bounds
    roi_cx = max(roi_size/2, min(1.0 - roi_size/2, cx))
    roi_cy = max(roi_size/2, min(1.0 - roi_size/2, roi_cy))

    return (roi_cx, roi_cy, roi_size)


# ============================================================
# Main Event Loop
# ============================================================
while True:
    # 1. Receive Palm Detection Output
    nn_data = node.io['palm_in'].get()

    # 2. Decode SSD Outputs
    detections = decode_palm_detections(nn_data, conf_threshold=0.5)

    # 3. Apply NMS
    detections = apply_nms(detections, iou_threshold=0.3)

    # 4. Select best detection (highest confidence)
    if detections:
        best_palm = detections[0]  # Already sorted by confidence

        # Calculate ROI for Landmark model
        roi_cx, roi_cy, roi_size = calculate_roi(best_palm)

        # Convert normalized coords to ImageManip config
        cfg = ImageManipConfig()

        # CRITICAL: Use setKeepAspectRatio + setCropRect for dynamic ROI
        # This crops the region THEN resizes to 224x224

        # Convert normalized [0,1] to pixel coords [0, 1920x1080] (full RGB frame)
        # Note: ImageManip receives full RGB frame from Camera (not 192x192 preview)
        INPUT_WIDTH = 1920  # RGB Camera resolution
        INPUT_HEIGHT = 1080

        # Calculate crop rectangle (top-left corner + size)
        crop_x = int((roi_cx - roi_size/2) * INPUT_WIDTH)
        crop_y = int((roi_cy - roi_size/2) * INPUT_HEIGHT)
        crop_w = int(roi_size * INPUT_WIDTH)
        crop_h = int(roi_size * INPUT_HEIGHT)

        # Clamp to image bounds
        crop_x = max(0, min(INPUT_WIDTH - crop_w, crop_x))
        crop_y = max(0, min(INPUT_HEIGHT - crop_h, crop_y))
        crop_w = min(INPUT_WIDTH - crop_x, crop_w)
        crop_h = min(INPUT_HEIGHT - crop_y, crop_h)

        # Calculate normalized center and size for setCenterCrop
        # DepthAI Script API uses setCenterCrop(ratio_x, ratio_y) NOT setCropRect!
        center_x = (crop_x + crop_w / 2.0) / INPUT_WIDTH
        center_y = (crop_y + crop_h / 2.0) / INPUT_HEIGHT
        crop_ratio = max(crop_w / INPUT_WIDTH, crop_h / INPUT_HEIGHT)

        cfg.setCenterCrop(crop_ratio, crop_ratio)
        cfg.setResize(224, 224)
        cfg.setKeepAspectRatio(False)  # Stretch to 224x224

    else:
        # No palm detected: Fallback to center crop
        cfg = ImageManipConfig()
        cfg.setCenterCrop(0.5, 0.5)  # 50% center crop
        cfg.setResize(224, 224)
        cfg.setKeepAspectRatio(False)

    # 5. Send config to ImageManip
    node.io['manip_cfg'].send(cfg)

    # 6. Pass-through palm data for host synchronization
    node.io['palm_pass'].send(nn_data)

